import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np
import os

from utils.cv_reader import read_resume_from_file
from utils.github_reader import extract_github_links_from_text, collect_github_text
from utils.resume_processor import preprocess_text
from utils.constants import competency_list, profession_matrix, profession_names


# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
@st.cache_resource
def load_model():
    model_path = "model"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()


# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
def predict_competencies(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()
    binary_preds = (probs > 0.5).astype(int)
    return binary_preds, probs


# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit
st.set_page_config(page_title="AI –†–µ–∑—é–º–µ –ê–Ω–∞–ª–∏–∑", layout="wide")
st.title("üíº AI –ê–Ω–∞–ª–∏–∑ –†–µ–∑—é–º–µ –∏ –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π")

uploaded_file = st.file_uploader("üì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∞—à–µ —Ä–µ–∑—é–º–µ (PDF, DOCX, TXT)", type=["pdf", "docx", "txt"])
if uploaded_file:
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    with open("temp_file", "wb") as f:
        f.write(uploaded_file.read())

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—é–º–µ + GitHub
    base_text = read_resume_from_file("temp_file")
    gh_links = extract_github_links_from_text(base_text)
    github_text = " ".join([collect_github_text(link) for link in gh_links]) if gh_links else ""
    full_text = preprocess_text(base_text + " " + github_text)

    # üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    st.subheader("üß† –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏:")
    pred_vector, prob_vector = predict_competencies(full_text)
    detected_competencies = [competency_list[i] for i, val in enumerate(pred_vector) if val == 1]

    for comp in detected_competencies:
        st.markdown(f"- ‚úÖ {comp}")

    st.markdown("---")
    st.subheader("üìà –£–∫–∞–∂–∏—Ç–µ —Å–≤–æ–π –≥—Ä–µ–π–¥ (0‚Äì3) –ø–æ –∫–∞–∂–¥–æ–π –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏:")

    user_grades = []
    for i, comp in enumerate(competency_list):
        if pred_vector[i] == 1:
            grade = st.selectbox(f"{comp}", options=[0, 1, 2, 3], index=1, key=f"grade_{i}")
        else:
            grade = 0
        user_grades.append(grade)

    st.markdown("---")
    st.subheader("üß© –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º:")

    user_vector = np.array(user_grades)
    for prof_idx in range(len(profession_names)):
        prof_required = profession_matrix[:, prof_idx]
        total_required = np.sum(prof_required > 0)
        matched = np.sum((user_vector >= prof_required) & (prof_required > 0))
        percent = (matched / total_required) * 100 if total_required > 0 else 0
        st.write(f"üîπ **{profession_names[prof_idx]}**: {percent:.1f}% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è")

    st.markdown("---")
    st.subheader("üîÆ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ –ø–æ–∑–∂–µ)")
    st.info("–ó–¥–µ—Å—å –≤ –±—É–¥—É—â–µ–º –ø–æ—è–≤—è—Ç—Å—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–∞—Ä—å–µ—Ä–Ω—ã–º —Ç—Ä–µ–∫–∞–º –∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –≤–∞–∫–∞–Ω—Å–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–∏—Ö –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π.")

    # –ü—Ä–æ—Å–º–æ—Ç—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    with st.expander("üßæ –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ"):
        st.text(full_text)

    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    os.remove("temp_file")