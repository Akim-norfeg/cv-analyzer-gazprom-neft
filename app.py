import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np
import os

from utils.cv_reader import read_resume_from_file
from utils.github_reader import extract_github_links_from_text, collect_github_text
from utils.resume_processor import preprocess_text
from utils.constants import competency_list, profession_matrix, profession_names

@st.cache_resource
def load_model():
    repo_id = "KsyLight/resume-ai-competency-model"
    tokenizer = AutoTokenizer.from_pretrained(repo_id)
    model = AutoModelForSequenceClassification.from_pretrained(repo_id)
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()

def predict_competencies(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()
    binary_preds = (probs > 0.5).astype(int)
    return binary_preds, probs

st.set_page_config(page_title="AI –†–µ–∑—é–º–µ –ê–Ω–∞–ª–∏–∑", layout="wide")
st.title("üíº AI –ê–Ω–∞–ª–∏–∑ –†–µ–∑—é–º–µ –∏ –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π")

uploaded_file = st.file_uploader("üì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∞—à–µ —Ä–µ–∑—é–º–µ (PDF, DOCX, TXT)", type=["pdf", "docx", "txt"])
if uploaded_file:
    with st.spinner("‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞..."):
        with open("temp_file", "wb") as f:
            f.write(uploaded_file.read())

        base_text = read_resume_from_file("temp_file")
        gh_links = extract_github_links_from_text(base_text)
        github_text = " ".join([collect_github_text(link) for link in gh_links]) if gh_links else ""
        full_text = preprocess_text(base_text + " " + github_text)

    with st.spinner("ü§ñ –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏..."):
        pred_vector, prob_vector = predict_competencies(full_text)

    st.subheader("üß† –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏:")
    for i, prob in enumerate(prob_vector):
        if pred_vector[i] == 1:
            comp = competency_list[i]
            st.markdown(f"- ‚úÖ {comp} ‚Äî **{prob:.2f}**")

    st.markdown("---")
    st.subheader("üìà –£–∫–∞–∂–∏—Ç–µ —Å–≤–æ–π –≥—Ä–µ–π–¥ (0‚Äì3) –ø–æ –∫–∞–∂–¥–æ–π –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏:")

    user_grades = []
    for i, comp in enumerate(competency_list):
        grade = st.selectbox(f"{comp}", options=[0, 1, 2, 3], index=1 if pred_vector[i] == 1 else 0, key=f"grade_{i}")
        user_grades.append(grade)

    st.markdown("---")
    st.subheader("üß© –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º:")

    user_vector = np.array(user_grades)
    for prof_idx in range(len(profession_names)):
        prof_required = profession_matrix[:, prof_idx]
        total_required = np.sum(prof_required > 0)
        matched = np.sum((user_vector >= prof_required) & (prof_required > 0))
        percent = (matched / total_required) * 100 if total_required > 0 else 0
        st.write(f"üîπ **{profession_names[prof_idx]}**: {percent:.1f}% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è")

    st.markdown("---")
    st.subheader("üîÆ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ –ø–æ–∑–∂–µ)")
    st.info("–ó–¥–µ—Å—å –≤ –±—É–¥—É—â–µ–º –ø–æ—è–≤—è—Ç—Å—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–∞—Ä—å–µ—Ä–Ω—ã–º —Ç—Ä–µ–∫–∞–º –∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –≤–∞–∫–∞–Ω—Å–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–∏—Ö –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π.")

    with st.expander("üßæ –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ"):
        st.text(full_text)

    os.remove("temp_file")