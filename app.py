import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import login
import torch
import numpy as np
import tempfile
import os
import logging
import matplotlib.pyplot as plt
import seaborn as sns

from utils.cv_reader import read_resume_from_file
from utils.github_reader import extract_github_links_from_text, collect_github_text
from utils.resume_processor import preprocess_text
from utils.constants import competency_list, profession_matrix, profession_names

# üåû –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–≤–µ—Ç–ª–æ–π —Ç–µ–º—ã
st.set_page_config(page_title="AI –†–µ–∑—é–º–µ –ê–Ω–∞–ª–∏–∑", layout="wide", initial_sidebar_state="expanded")

# üìù –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
os.makedirs("logs", exist_ok=True)
logging.basicConfig(filename="logs/errors.log", level=logging.ERROR,
                    format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s")

st.title("üíº AI –ê–Ω–∞–ª–∏–∑ –†–µ–∑—é–º–µ –∏ –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π")

# üîê –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
@st.cache_resource
def load_model():
    login(token=st.secrets["HUGGINGFACE_TOKEN"])
    repo_id = "KsyLight/resume-ai-competency-model"
    tokenizer = AutoTokenizer.from_pretrained(repo_id, use_auth_token=True)
    model = AutoModelForSequenceClassification.from_pretrained(repo_id, use_auth_token=True)
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()

# ü§ñ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
def predict_competencies(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()
    binary_preds = (probs > 0.5).astype(int)
    return binary_preds, probs

# üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
uploaded_file = st.file_uploader("üì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∞—à–µ —Ä–µ–∑—é–º–µ (PDF, DOCX, TXT)", type=["pdf", "docx", "txt"])

if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_file_path = tmp_file.name

    try:
        with st.spinner("‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ..."):
            base_text = read_resume_from_file(tmp_file_path)
            if not base_text:
                st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª.")
                st.stop()

            gh_links = extract_github_links_from_text(base_text)
            github_text = ""
            if gh_links:
                st.markdown("üîó <b>–ù–∞–π–¥–µ–Ω–Ω—ã–µ GitHub-—Å—Å—ã–ª–∫–∏:</b>", unsafe_allow_html=True)
                for link in gh_links:
                    st.markdown(f"- [{link}]({link})")
                    try:
                        github_text += " " + preprocess_text(collect_github_text(link))
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ {link}")
                        logging.error(f"GitHub fetch error ({link}): {e}")

            full_text = preprocess_text(base_text + " " + github_text)
            pred_vector, prob_vector = predict_competencies(full_text)

        # üß† –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
        st.subheader("üß† –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏:")
        for i, prob in enumerate(prob_vector):
            if pred_vector[i] == 1:
                st.markdown(f"- ‚úÖ {competency_list[i]} ‚Äî **{prob:.2f}**")

        # üß™ –ì—Ä–µ–π–¥—ã (radio-–∫–Ω–æ–ø–∫–∏)
        st.markdown("---")
        st.subheader("üìà –£–∫–∞–∂–∏—Ç–µ –≤–∞—à –≥—Ä–µ–π–¥ –ø–æ –∫–∞–∂–¥–æ–π –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏:")
        user_grades = []
        for i, comp in enumerate(competency_list):
            default = 1 if pred_vector[i] == 1 else 0
            grade = st.radio(comp, [0, 1, 2, 3], index=default, horizontal=True, key=f"grade_{i}")
            user_grades.append(grade)

        user_vector = np.array(user_grades)

        # üëî –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º
        st.markdown("---")
        st.subheader("üß© –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º:")
        results = []
        for prof_idx, prof_name in enumerate(profession_names):
            prof_required = profession_matrix[:, prof_idx]
            total_required = np.sum(prof_required > 0)
            matched = np.sum((user_vector >= prof_required) & (prof_required > 0))
            percent = (matched / total_required) * 100 if total_required > 0 else 0
            st.write(f"üîπ **{prof_name}**: {percent:.1f}% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è")
            results.append(percent)

        # üé® –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è (heatmap)
        st.markdown("### üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è")
        fig, ax = plt.subplots(figsize=(6, 1.5))
        sns.heatmap([results], annot=True, fmt=".1f", cmap="YlGnBu", xticklabels=profession_names, yticklabels=["%"])
        st.pyplot(fig)

        # üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–±—É–¥—É—â–µ–µ)
        st.markdown("---")
        st.subheader("üîÆ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ)")
        st.info("–°–∫–æ—Ä–æ –∑–¥–µ—Å—å –ø–æ—è–≤—è—Ç—Å—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –∫–∞—Ä—å–µ—Ä–Ω—ã–µ —Å–æ–≤–µ—Ç—ã –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.")

        # üßæ –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
        with st.expander("üìÉ –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ –∏ GitHub:"):
            st.text(full_text)

    except Exception as e:
        st.error("üö´ –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É.")
        logging.error(f"Unexpected error: {e}")
    finally:
        os.remove(tmp_file_path)