import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import login
import torch
import numpy as np
import os
import logging
import matplotlib.pyplot as plt
import seaborn as sns

from utils.cv_reader import read_resume_from_file, preprocess_text
from utils.github_reader import extract_github_links_from_text, collect_github_text
from utils.constants import competency_list, profession_matrix, profession_names

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="AI –†–µ–∑—é–º–µ –ê–Ω–∞–ª–∏–∑",
    layout="wide",
    initial_sidebar_state="collapsed"
)
st.title("üíº AI –ê–Ω–∞–ª–∏–∑ –†–µ–∑—é–º–µ –∏ –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π")

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
os.makedirs("logs", exist_ok=True)
logging.basicConfig(
    filename="logs/errors.log",
    level=logging.ERROR,
    format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s"
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
@st.cache_resource
def load_model():
    login(token=st.secrets["HUGGINGFACE_TOKEN"])
    repo_id = "KsyLight/resume-ai-competency-model"
    tokenizer = AutoTokenizer.from_pretrained(repo_id, token=st.secrets["HUGGINGFACE_TOKEN"])
    model = AutoModelForSequenceClassification.from_pretrained(repo_id, token=st.secrets["HUGGINGFACE_TOKEN"])
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()

def predict_competencies(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()
    binary_preds = (probs > 0.5).astype(int)
    return binary_preds, probs

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
uploaded_file = st.file_uploader("üì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ (PDF, DOCX, TXT)", type=["pdf", "docx", "txt"])

if uploaded_file:
    os.makedirs("temp", exist_ok=True)
    tmp_file_path = os.path.join("temp", uploaded_file.name)

    with open(tmp_file_path, "wb") as f:
        f.write(uploaded_file.read())

    try:
        with st.spinner("‚è≥ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞..."):
            base_text = read_resume_from_file(tmp_file_path)
            if not base_text:
                st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç.")
                st.stop()

            gh_links = extract_github_links_from_text(base_text)
            github_text = ""
            if gh_links:
                st.markdown("üîó <b>GitHub-—Å—Å—ã–ª–∫–∏:</b>", unsafe_allow_html=True)
                for link in gh_links:
                    st.markdown(f"- [{link}]({link})")
                    try:
                        github_text += " " + preprocess_text(collect_github_text(link))
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {link}")
                        logging.error(f"GitHub fetch error ({link}): {e}")

            full_text = preprocess_text(base_text + " " + github_text)

        with st.spinner("ü§ñ –ê–Ω–∞–ª–∏–∑..."):
            pred_vector, prob_vector = predict_competencies(full_text)

        # –í–∫–ª–∞–¥–∫–∏
        tab1, tab2, tab3 = st.tabs(["üìã –û–ø—Ä–æ—Å", "üìä –ü—Ä–æ—Ñ–µ—Å—Å–∏–∏", "üìÑ –†–µ–∑—é–º–µ"])

        # –í–∫–ª–∞–¥–∫–∞ –û–ø—Ä–æ—Å
        with tab1:
            st.subheader("üìà –í–∞—à —É—Ä–æ–≤–µ–Ω—å –≤–ª–∞–¥–µ–Ω–∏—è –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º (0‚Äì3):")
            user_grades = []
            for i, comp in enumerate(competency_list):
                default = 1 if pred_vector[i] else 0
                grade = st.radio(comp, [0, 1, 2, 3], index=default, horizontal=True, key=f"grade_{i}")
                user_grades.append(grade)

            st.session_state.user_grades = user_grades
            st.success("‚úÖ –ì—Ä–µ–π–¥—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤–æ –≤–∫–ª–∞–¥–∫—É '–ü—Ä–æ—Ñ–µ—Å—Å–∏–∏'")

        # –í–∫–ª–∞–¥–∫–∞ –ü—Ä–æ—Ñ–µ—Å—Å–∏–∏
        with tab2:
            if "user_grades" not in st.session_state:
                st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –≥—Ä–µ–π–¥—ã –≤–æ –≤–∫–ª–∞–¥–∫–µ '–û–ø—Ä–æ—Å'")
                st.stop()

            user_vector = np.array(st.session_state.user_grades)

            if len(user_vector) != profession_matrix.shape[0]:
                st.error("‚ö†Ô∏è –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –º–∞—Ç—Ä–∏—Ü–µ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–π.")
                logging.error(f"user_vector={len(user_vector)}, matrix_rows={profession_matrix.shape[0]}")
                st.stop()

            col1, col2 = st.columns(2)

            # –õ–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞: –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ —Å –≥—Ä–µ–π–¥–∞–º–∏
            with col1:
                st.markdown("### üß† –í–∞—à–∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –∏ –≥—Ä–µ–π–¥—ã:")
                for comp, grade in zip(competency_list, user_vector):
                    st.markdown(f"- **{comp}**: {grade}")

            # –ü—Ä–∞–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞: –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º + –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            with col2:
                st.markdown("### üëî –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º")
                percentages = []
                for i, prof in enumerate(profession_names):
                    required = profession_matrix[:, i]
                    matched = np.sum((user_vector >= required) & (required > 0))
                    total = np.sum(required > 0)
                    percent = (matched / total) * 100 if total else 0
                    percentages.append(percent)
                    st.markdown(f"üîπ **{prof}** ‚Äî {percent:.1f}% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è")

                st.markdown("### üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ –≤–∏–¥–µ –∫—Ä—É–≥–æ–≤–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã")
                fig, ax = plt.subplots()
                colors = sns.color_palette("pastel")[0:len(profession_names)]
                ax.pie(percentages, labels=profession_names, autopct="%1.1f%%", startangle=90, colors=colors)
                ax.axis("equal")
                st.pyplot(fig)

            # –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π
            st.markdown("### üìò –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π")
            profession_descriptions = {
                "–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö": "–ò–∑—É—á–∞–µ—Ç –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ, –ø—Ä–∏–º–µ–Ω—è–µ—Ç ML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.",
                "–ò–Ω–∂–µ–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö": "–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ö—Ä–∞–Ω–µ–Ω–∏–µ, –æ—á–∏—Å—Ç–∫—É, –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –∏ –ø–µ—Ä–µ–¥–∞—á—É –¥–∞–Ω–Ω—ã—Ö.",
                "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤ –ò–ò": "–°–≤—è–∑—ã–≤–∞–µ—Ç –±–∏–∑–Ω–µ—Å –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ò–ò, –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è.",
                "–ú–µ–Ω–µ–¥–∂–µ—Ä –≤ –ò–ò": "–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –ò–ò –∏ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –∫–æ–º–∞–Ω–¥—É."
            }
            for prof, desc in profession_descriptions.items():
                st.markdown(f"**{prof}** ‚Äî {desc}")

        # –í–∫–ª–∞–¥–∫–∞ –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
        with tab3:
            st.markdown("### üìÑ –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ")
            st.text(full_text)

    except Exception as e:
        st.error("üö´ –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª.")
        logging.error(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞: {e}")

    finally:
        if os.path.exists(tmp_file_path):
            os.remove(tmp_file_path)