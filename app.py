import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import login
import torch
import numpy as np
import os
import logging
import matplotlib.pyplot as plt
import seaborn as sns

from utils.cv_reader import read_resume_from_file, preprocess_text
from utils.github_reader import extract_github_links_from_text, collect_github_text
from utils.constants import competency_list, profession_matrix, profession_names

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="AI –†–µ–∑—é–º–µ –ê–Ω–∞–ª–∏–∑", layout="wide")

# –ö–∞—Å—Ç–æ–º–Ω–∞—è —Å–≤–µ—Ç–ª–∞—è —Ç–µ–º–∞
st.markdown("""
    <style>
        .main { background-color: #f9f9f9; }
        .stRadio > div { flex-direction: row; }
        h1, h2, h3 { color: #0d3b66; }
        .block-container { padding-top: 2rem; padding-bottom: 2rem; }
    </style>
""", unsafe_allow_html=True)

st.title("üíº AI –ê–Ω–∞–ª–∏–∑ –†–µ–∑—é–º–µ –∏ –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π")

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
os.makedirs("logs", exist_ok=True)
logging.basicConfig(filename="logs/errors.log", level=logging.ERROR, 
                    format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
@st.cache_resource
def load_model():
    login(token=st.secrets["HUGGINGFACE_TOKEN"])
    repo_id = "KsyLight/resume-ai-competency-model"
    tokenizer = AutoTokenizer.from_pretrained(repo_id, token=st.secrets["HUGGINGFACE_TOKEN"])
    model = AutoModelForSequenceClassification.from_pretrained(repo_id, token=st.secrets["HUGGINGFACE_TOKEN"])
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
def predict_competencies(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()
    binary_preds = (probs > 0.5).astype(int)
    return binary_preds, probs

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å –≤–∫–ª–∞–¥–∫–∞–º–∏
tab1, tab2, tab3 = st.tabs(["üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—é–º–µ", "üß† –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏", "üëî –ü—Ä–æ—Ñ–µ—Å—Å–∏–∏"])

with tab1:
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ (PDF, DOCX, TXT)", type=["pdf", "docx", "txt"])
    if uploaded_file:
        os.makedirs("temp", exist_ok=True)
        tmp_file_path = os.path.join("temp", uploaded_file.name)

        with open(tmp_file_path, "wb") as f:
            f.write(uploaded_file.read())

        try:
            with st.spinner("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞..."):
                base_text = read_resume_from_file(tmp_file_path)
                if not base_text:
                    st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç.")
                    st.stop()

                gh_links = extract_github_links_from_text(base_text)
                github_text = ""
                if gh_links:
                    st.markdown("üîó <b>GitHub-—Å—Å—ã–ª–∫–∏:</b>", unsafe_allow_html=True)
                    for link in gh_links:
                        st.markdown(f"- [{link}]({link})")
                        try:
                            github_text += " " + preprocess_text(collect_github_text(link))
                        except Exception as e:
                            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {link}")
                            logging.error(f"GitHub fetch error ({link}): {e}")

                full_text = preprocess_text(base_text + " " + github_text)

            st.session_state["resume_text"] = full_text
            st.success("‚úÖ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤–æ –≤–∫–ª–∞–¥–∫—É üß† –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏")

        except Exception as e:
            st.error("üö´ –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞.")
            logging.error(f"–û—à–∏–±–∫–∞: {e}")

        finally:
            if os.path.exists(tmp_file_path):
                os.remove(tmp_file_path)

with tab2:
    if "resume_text" not in st.session_state:
        st.info("‚¨ÖÔ∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ –≤–æ –≤–∫–ª–∞–¥–∫–µ 'üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—é–º–µ'")
        st.stop()

    text = st.session_state["resume_text"]
    pred_vector, prob_vector = predict_competencies(text)

    st.subheader("üß† –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏:")
    for i, prob in enumerate(prob_vector):
        if pred_vector[i]:
            st.markdown(f"- ‚úÖ **{competency_list[i]}** ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: **{prob:.2f}**")

    st.subheader("üìà –£–∫–∞–∂–∏—Ç–µ –≤–∞—à —É—Ä–æ–≤–µ–Ω—å (0‚Äì3):")
    user_grades = []
    for i, comp in enumerate(competency_list):
        default = 1 if pred_vector[i] else 0
        try:
            grade = st.radio(comp, [0, 1, 2, 3], index=default, horizontal=True, key=f"grade_{i}")
            user_grades.append(grade)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –≥—Ä–µ–π–¥–∞: {e}")
            st.error(f"–û—à–∏–±–∫–∞ –¥–ª—è: {comp}")
            user_grades.append(0)

    if len(user_grades) != profession_matrix.shape[0]:
        st.error("‚ùå –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –º–∞—Ç—Ä–∏—Ü–µ–π.")
        st.stop()

    st.session_state["user_vector"] = np.array(user_grades)
    st.success("‚úÖ –ì—Ä–µ–π–¥—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤–æ –≤–∫–ª–∞–¥–∫—É üëî –ü—Ä–æ—Ñ–µ—Å—Å–∏–∏")

with tab3:
    if "user_vector" not in st.session_state:
        st.info("‚¨ÖÔ∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–∫–∞–∂–∏—Ç–µ –≥—Ä–µ–π–¥—ã –≤–æ –≤–∫–ª–∞–¥–∫–µ 'üß† –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏'")
        st.stop()

    st.subheader("üëî –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º")
    percentages = []
    user_vector = st.session_state["user_vector"]

    for i, prof in enumerate(profession_names):
        required = profession_matrix[:, i]
        matched = np.sum((user_vector >= required) & (required > 0))
        total = np.sum(required > 0)
        percent = (matched / total) * 100 if total else 0
        percentages.append(percent)
        st.write(f"üîπ **{prof}** ‚Äî {percent:.1f}% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è")

    # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
    st.markdown("### üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è")
    fig, ax = plt.subplots(figsize=(8, 1.5))
    sns.heatmap([percentages], annot=True, fmt=".1f", cmap="Blues", cbar=False,
                xticklabels=profession_names, yticklabels=["% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è"])
    st.pyplot(fig)

    st.markdown("---")
    st.subheader("üîÆ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–±—É–¥–µ—Ç –ø–æ–∑–∂–µ)")
    st.info("–ó–¥–µ—Å—å –ø–æ—è–≤—è—Ç—Å—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –æ–±—É—á–µ–Ω–∏—é –∏ –∫–∞—Ä—å–µ—Ä–Ω–æ–º—É —Ä–æ—Å—Ç—É.")