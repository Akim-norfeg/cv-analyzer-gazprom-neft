import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import login
import torch
import numpy as np
import os
import logging
import matplotlib.pyplot as plt
import seaborn as sns
import mplcyberpunk

plt.style.use('cyberpunk')

from utils.cv_reader import read_resume_from_file, preprocess_text
from utils.github_reader import extract_github_links_from_text, collect_github_text
from utils.constants import competency_list, profession_matrix, profession_names

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ –ø–æ –º–∞—Ç—Ä–∏—Ü–µ –ê–ª—å—è–Ω—Å–∞ –ò–ò",
    layout="wide",
    initial_sidebar_state="collapsed"
)
st.title("–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ –ø–æ –º–∞—Ç—Ä–∏—Ü–µ –ê–ª—å—è–Ω—Å–∞ –ò–ò")

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
os.makedirs("logs", exist_ok=True)
logging.basicConfig(
    filename="logs/errors.log",
    level=logging.ERROR,
    format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s"
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
@st.cache_resource
def load_model():
    login(token=st.secrets["HUGGINGFACE_TOKEN"])
    repo_id = "KsyLight/resume-ai-competency-model"
    tokenizer = AutoTokenizer.from_pretrained(repo_id, token=st.secrets["HUGGINGFACE_TOKEN"])
    model = AutoModelForSequenceClassification.from_pretrained(repo_id, token=st.secrets["HUGGINGFACE_TOKEN"])
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
def predict_competencies(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()
    binary_preds = (probs > 0.5).astype(int)
    return binary_preds, probs

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
uploaded_file = st.file_uploader("üì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ (PDF, DOCX, TXT)", type=["pdf", "docx", "txt"])

if uploaded_file:
    os.makedirs("temp", exist_ok=True)
    tmp_file_path = os.path.join("temp", uploaded_file.name)

    with open(tmp_file_path, "wb") as f:
        f.write(uploaded_file.read())

    try:
        with st.spinner("‚è≥ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞..."):
            base_text = read_resume_from_file(tmp_file_path)
            if not base_text:
                st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç.")
                st.stop()

            gh_links = extract_github_links_from_text(base_text)
            github_text_raw = ""
            if gh_links:
                st.markdown("üîó <b>GitHub-—Å—Å—ã–ª–∫–∏:</b>", unsafe_allow_html=True)
                for link in gh_links:
                    st.markdown(f"- [{link}]({link})")
                    try:
                        github_text_raw += " " + collect_github_text(link)
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {link}")
                        logging.error(f"GitHub fetch error ({link}): {e}")

            full_text = preprocess_text(base_text + " " + github_text_raw)

        with st.spinner("ü§ñ –ê–Ω–∞–ª–∏–∑..."):
            pred_vector, prob_vector = predict_competencies(full_text)

        # –í–∫–ª–∞–¥–∫–∏
        tab1, tab2, tab3 = st.tabs(["–û–ø—Ä–æ—Å", "–ü—Ä–æ—Ñ–µ—Å—Å–∏–∏", "–†–µ–∑—é–º–µ"])

        # –í–∫–ª–∞–¥–∫–∞ –û–ø—Ä–æ—Å (–¥–≤–µ –∫–æ–ª–æ–Ω–∫–∏)
        with tab1:
            st.subheader("–í–∞—à —É—Ä–æ–≤–µ–Ω—å –≤–ª–∞–¥–µ–Ω–∏—è –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º (0‚Äì3):")
            user_grades = []
            col1, col2 = st.columns(2)

            for i, comp in enumerate(competency_list):
                default = 1 if pred_vector[i] else 0
                with col1 if i % 2 == 0 else col2:
                    grade = st.radio(comp, [0, 1, 2, 3], index=default, horizontal=True, key=f"grade_{i}")
                    user_grades.append(grade)

            st.session_state.user_grades = user_grades
            st.success("‚úÖ –ì—Ä–µ–π–¥—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤–æ –≤–∫–ª–∞–¥–∫—É '–ü—Ä–æ—Ñ–µ—Å—Å–∏–∏'")

        # –í–∫–ª–∞–¥–∫–∞ –ü—Ä–æ—Ñ–µ—Å—Å–∏–∏
        with tab2:
            if "user_grades" not in st.session_state:
                st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –≥—Ä–µ–π–¥—ã –≤–æ –≤–∫–ª–∞–¥–∫–µ '–û–ø—Ä–æ—Å'")
                st.stop()

            user_vector = np.array(st.session_state.user_grades)

            if len(user_vector) != profession_matrix.shape[0]:
                st.error("‚ö†Ô∏è –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –º–∞—Ç—Ä–∏—Ü–µ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–π.")
                logging.error(f"user_vector={len(user_vector)}, matrix_rows={profession_matrix.shape[0]}")
                st.stop()

            col1, col2 = st.columns(2)

            with col1:
                st.markdown("### –í–∞—à–∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –∏ –≥—Ä–µ–π–¥—ã:")

                # –õ–µ–≥–µ–Ω–¥–∞
                st.markdown("""
                <div style='font-size: 15px; margin-bottom: 10px;'>
                    <b>üü© ‚Äî –≥—Ä–µ–π–¥ 3</b> (–≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å)<br>
                    <b>üü® ‚Äî –≥—Ä–µ–π–¥ 2</b> (—É–≤–µ—Ä–µ–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å)<br>
                    <b>üü¶ ‚Äî –≥—Ä–µ–π–¥ 1</b> (–Ω–∞—á–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å)<br>
                    <b>‚¨úÔ∏è ‚Äî –≥—Ä–µ–π–¥ 0</b> (–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç)
                </div>
                """, unsafe_allow_html=True)

                # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≥—Ä–µ–π–¥—É
                sorted_competencies = sorted(zip(competency_list, user_vector), key=lambda x: -x[1])
                for comp, grade in sorted_competencies:
                    color = {3: "üü©", 2: "üü®", 1: "üü¶", 0: "‚¨úÔ∏è"}.get(grade, "‚¨úÔ∏è")
                    st.markdown(f"{color} **{comp}** ‚Äî –≥—Ä–µ–π–¥: **{grade}**")

            with col2:
                st.markdown("### –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º")

                # –†–∞—Å—á—ë—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
                percentages = []
                for i, prof in enumerate(profession_names):
                    required = profession_matrix[:, i]
                    matched = np.sum((user_vector >= required) & (required > 0))
                    total = np.sum(required > 0)
                    percent = (matched / total) * 100 if total else 0
                    percentages.append((prof, percent))

                sorted_percentages = sorted(percentages, key=lambda x: x[1], reverse=True)

                for prof, percent in sorted_percentages:
                    st.markdown(f"üîπ **{prof}** ‚Äî {percent:.1f}% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è")

                # –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
                st.markdown("### –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞")
                fig, ax = plt.subplots()
                labels = [prof for prof, _ in sorted_percentages]
                values = [percent for _, percent in sorted_percentages]
                colors = sns.color_palette("pastel")[0:len(sorted_percentages)]

                ax.pie(values, labels=labels, autopct="%1.1f%%", startangle=90, colors=colors)
                ax.axis("equal")
                mplcyberpunk.add_glow_effects()
                st.pyplot(fig)

                # –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π –≤ —Ç–∞–±–ª–∏—Ü–µ
                st.markdown("### –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π")
                descriptions = {
                    "–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö (Data scientist, ML engineer)": """–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–æ–º–ø–∞–Ω–∏–∏, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Ö –∏ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ò–ò. –°–æ–≤–º–µ—Å—Ç–Ω–æ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞–º–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫.

                ‚Ä¢ –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª—É—á—à–∏–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–ø–æ—Å–æ–± –µ–≥–æ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ —Å–ø–µ—Ü–∏—Ñ–∏–∫–µ –∑–∞–¥–∞—á–∏  
                ‚Ä¢ –†–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (feature-engineering)  
                ‚Ä¢ –†–µ–∞–ª–∏–∑—É–µ—Ç –æ–±—â–∏–π –ø–∞–π–ø–ª–∞–π–Ω —Ä–µ—à–µ–Ω–∏—è  
                ‚Ä¢ –§–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é —á–∞—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞""",

                    "–ú–µ–Ω–µ–¥–∂–µ—Ä –≤ –ò–ò (Manager in AI)": """–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ–±—â–µ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞, —Ä–∞–±–æ—Ç—É –ø–æ –±—é–¥–∂–µ—Ç—É, —Ä–µ—Å—É—Ä—Å–∞–º, —Å—Ä–æ–∫–∞–º.  
                –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –∫–æ–Ω–≤–µ—Ä—Å–∏—é –∏ –≤—ã–≤–æ–¥ —Ä–µ—à–µ–Ω–∏–π –≤ –ø—Ä–æ–¥—É–∫—Ç–∏–≤ –Ω–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–º —É—Ä–æ–≤–Ω–µ. –¢–∞–∫–∂–µ –≤ –∫—Ä—É–≥ –µ–≥–æ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π –º–æ–∂–µ—Ç –≤—Ö–æ–¥–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤ –∏ —á–∞—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞.""",

                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤ –ò–ò (Technical analyst in AI)": """–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–º –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–∫–∞–∑—á–∏–∫–æ–º.  
                –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –±–∏–∑–Ω–µ—Å–∞, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –∏ —É—Ç–æ—á–Ω—è–µ—Ç –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏–∫—É, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å—ã –∏ –≤—ã—è–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –¥–∞–Ω–Ω—ã—Ö –≤ –Ω–∏—Ö.  
                –¢–∞–∫–∂–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é —Ä–µ–∞–ª–∏–∑—É–µ–º–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞, —Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ –∏ –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –º–æ–∂–µ—Ç —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.""",

                    "–ò–Ω–∂–µ–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö (Data engineer)": """–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Å–±–æ—Ä, –∞–Ω–∞–ª–∏–∑, –æ—á–∏—Å—Ç–∫—É –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.  
                –†–∞–±–æ—Ç–∞–µ—Ç —Å —Å–∏—Å—Ç–µ–º–∞–º–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –∞ —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∫—É —Å–∏—Å—Ç–µ–º –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö."""
                }

                # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–æ—Ü–µ–Ω—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
                sorted_professions = sorted(
                    zip(profession_names, percentages),
                    key=lambda x: -x[1]
                )

                # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–π —Å –ø–æ–ª–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ (—Å –∞–Ω–≥–ª. —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–æ–º)
                prof_name_mapping = {
                    "–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö": "–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö (Data scientist, ML engineer)",
                    "–ú–µ–Ω–µ–¥–∂–µ—Ä –≤ –ò–ò": "–ú–µ–Ω–µ–¥–∂–µ—Ä –≤ –ò–ò (Manager in AI)",
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤ –ò–ò": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤ –ò–ò (Technical analyst in AI)",
                    "–ò–Ω–∂–µ–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö": "–ò–Ω–∂–µ–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö (Data engineer)"
                }

                # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É
                table_data = {
                    "–ü—Ä–æ—Ñ–µ—Å—Å–∏—è": [],
                    "–û–ø–∏—Å–∞–Ω–∏–µ": []
                }

                for prof, _ in sorted_professions:
                    full_name = prof_name_mapping.get(prof, prof)
                    table_data["–ü—Ä–æ—Ñ–µ—Å—Å–∏—è"].append(full_name)
                    table_data["–û–ø–∏—Å–∞–Ω–∏–µ"].append(descriptions.get(full_name, "‚Äî"))

                st.dataframe(table_data, use_container_width=True)
                
        # –í–∫–ª–∞–¥–∫–∞ –†–µ–∑—é–º–µ
        with tab3:
            st.markdown("### –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ")
            with st.expander("üìù –¢–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ —Ä–µ–∑—é–º–µ"):
                st.text(base_text)

            if github_text_raw.strip():
                with st.expander("üßë‚Äçüíª –¢–µ–∫—Å—Ç, —Å–æ–±—Ä–∞–Ω–Ω—ã–π —Å GitHub"):
                    st.text(github_text_raw)
            else:
                st.info("GitHub-—Å—Å—ã–ª–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ.")

    except Exception as e:
        st.error("üö´ –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª.")
        logging.error(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞: {e}")

    finally:
        if os.path.exists(tmp_file_path):
            os.remove(tmp_file_path)
