{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11312580,"sourceType":"datasetVersion","datasetId":7075560}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import ast\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nimport optuna\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Загрузка и подготовка данных","metadata":{"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}}},{"cell_type":"code","source":"raw_dataset = load_dataset(\"csv\", data_files={\"data\": \"/kaggle/input/resume/resume_dataset.csv\"})\n\ndef process_labels(example):\n    example[\"labels\"] = ast.literal_eval(example[\"labels\"])\n    example[\"labels\"] = [float(x) for x in example[\"labels\"]]\n    return example\n\ndataset = raw_dataset[\"data\"].map(process_labels)\nprint(\"Dataset size:\", len(dataset))\n\nsplit_dataset = dataset.train_test_split(test_size=0.2, seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Токенизация","metadata":{"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}}},{"cell_type":"code","source":"model_checkpoint = \"xlm-roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\ndef tokenize_function(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n\ntokenized_datasets = split_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Вычисление весов классов","metadata":{"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}}},{"cell_type":"code","source":"def compute_class_weights(dataset):\n    total = len(dataset)\n    num_labels = len(dataset[0][\"labels\"])\n    label_sums = np.zeros(num_labels)\n    for example in dataset:\n        label_sums += np.array(example[\"labels\"])\n    epsilon = 1e-8\n    pos_weight = (total - label_sums) / (label_sums + epsilon)\n    return torch.tensor(pos_weight, dtype=torch.float)\n\npos_weight = compute_class_weights(tokenized_datasets[\"train\"])\nprint(\"pos_weight:\", pos_weight)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Инициализация модели","metadata":{"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}}},{"cell_type":"code","source":"def model_init():\n    config = AutoConfig.from_pretrained(model_checkpoint, num_labels=34, problem_type=\"multi_label_classification\")\n    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=config)\n\n# Глобальная переменная порога (будет обновляться в Optuna)\nTHRESHOLD = 0.45","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Функция вычисления метрик","metadata":{"execution":{"iopub.status.busy":"2025-04-17T21:14:30.320319Z","iopub.execute_input":"2025-04-17T21:14:30.320968Z","iopub.status.idle":"2025-04-17T21:14:30.324738Z","shell.execute_reply.started":"2025-04-17T21:14:30.320942Z","shell.execute_reply":"2025-04-17T21:14:30.323980Z"}}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    probs = 1 / (1 + np.exp(-logits))\n    predictions = (probs > THRESHOLD).astype(int)\n    micro_f1 = f1_score(labels, predictions, average=\"micro\", zero_division=0)\n    macro_f1 = f1_score(labels, predictions, average=\"macro\", zero_division=0)\n    # Вычисляем F1 по заданной формуле (используем eval_precision и eval_recall)\n    precision = precision_score(labels, predictions, average=\"micro\", zero_division=0)\n    recall = recall_score(labels, predictions, average=\"micro\", zero_division=0)\n    if precision + recall > 0:\n        f1_custom = 2 * (precision * recall) / (precision + recall)\n    else:\n        f1_custom = 0\n    return {\n         \"eval_micro_f1\": micro_f1,\n         \"eval_macro_f1\": macro_f1,\n         \"eval_f1\": f1_custom,  # Основной оптимизируемый показатель\n         \"eval_precision\": precision,\n         \"eval_recall\": recall\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Кастомный Trainer (без сохранения промежуточных чекпойнтов)","metadata":{"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}}},{"cell_type":"code","source":"class MyTrainer(Trainer):\n    def __init__(self, *args, pos_weight=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pos_weight = pos_weight\n        \n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        if \"labels\" in inputs:\n            inputs[\"labels\"] = inputs[\"labels\"].float()\n        outputs = model(**inputs)\n        logits = outputs.logits\n        pos_weight_device = self.pos_weight.to(logits.device) if self.pos_weight is not None else None\n        loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight_device)\n        loss = loss_fct(logits, inputs[\"labels\"])\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Оптимизация с помощью Optuna (1 trial, эпохи от 10 до 14)","metadata":{"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}}},{"cell_type":"code","source":"def objective(trial):\n    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1.5e-05, 2.5e-05)\n    weight_decay = trial.suggest_loguniform(\"weight_decay\", 0.001, 0.002)\n    batch_size = trial.suggest_categorical(\"batch_size\", [16])\n    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 10, 14)\n    threshold = trial.suggest_uniform(\"threshold\", 0.46, 0.48)\n    \n    global THRESHOLD\n    THRESHOLD = threshold\n    \n    training_args_trial = TrainingArguments(\n        output_dir=\"./results_trial\",\n        eval_strategy=\"epoch\",\n        save_strategy=\"no\",  # Не сохраняем промежуточные чекпойнты\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_train_epochs,\n        weight_decay=weight_decay,\n        warmup_steps=500,\n        logging_steps=10,\n        fp16=True,\n        report_to=\"none\"\n    )\n    \n    model = model_init()\n    \n    trainer_trial = MyTrainer(\n        model=model,\n        args=training_args_trial,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"test\"],\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n        pos_weight=pos_weight\n    )\n    \n    trainer_trial.train()\n    eval_results = trainer_trial.evaluate()\n    f1_custom = eval_results[\"eval_f1\"]\n    print(f\"Trial finished: f1: {f1_custom:.4f}, params: {trial.params}\")\n    return f1_custom\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=5)\n\nprint(\"Best trial:\")\nbest_trial = study.best_trial\nprint(\"  Value: \", best_trial.value)\nprint(\"  Params: \")\nfor key, value in best_trial.params.items():\n    print(f\"    {key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Обучение лучшей модели с найденными гиперпараметрами и сохранение только лучшей модели","metadata":{"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}}},{"cell_type":"code","source":"best_lr = best_trial.params[\"learning_rate\"]\nbest_wd = best_trial.params[\"weight_decay\"]\nbest_bs = best_trial.params[\"batch_size\"]\nbest_epochs = best_trial.params[\"num_train_epochs\"]\nbest_thresh = best_trial.params[\"threshold\"]\n\nTHRESHOLD = best_thresh\n\ntraining_args_best = TrainingArguments(\n    output_dir=\"./best_model_results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",  # Сохраняем чекпойнт после каждой эпохи\n    save_total_limit=1,     # Храним только лучший чекпойнт\n    learning_rate=best_lr,\n    per_device_train_batch_size=best_bs,\n    per_device_eval_batch_size=best_bs,\n    num_train_epochs=best_epochs,\n    weight_decay=best_wd,\n    warmup_steps=500,\n    logging_steps=10,\n    fp16=True,\n    load_best_model_at_end=True,  # После обучения загружается лучшая модель по метрике eval_f1\n    metric_for_best_model=\"eval_f1\",\n    greater_is_better=True,\n    report_to=\"none\"\n)\n\ntrainer_best = MyTrainer(\n    model_init=model_init,\n    args=training_args_best,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    pos_weight=pos_weight\n)\n\ntrainer_best.train()\nfinal_eval = trainer_best.evaluate()\nprint(\"Final evaluation results:\", final_eval)\n\n# Сохраняем только лучшую модель и токенизатор в папку \"Rock\"\nbest_model_save_path = \"./Rock\"\ntrainer_best.save_model(best_model_save_path)\ntokenizer.save_pretrained(best_model_save_path)\nprint(f\"Best model saved to {best_model_save_path}\")\n\n# Архивируем модель в zip-файл \"Rock.zip\"\n!zip -r Rock.zip \"{best_model_save_path}\"\nprint(\"Rock.zip created. Download it from the Output tab in Kaggle.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:18:47.866368Z","iopub.execute_input":"2025-04-07T21:18:47.866641Z","iopub.status.idle":"2025-04-07T21:52:45.411300Z","shell.execute_reply.started":"2025-04-07T21:18:47.866617Z","shell.execute_reply":"2025-04-07T21:52:45.410171Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating data split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f9197c804c24f9db43ac53ea5b7fc5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/727 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"435e46ee57d34999aa330651a7709b9b"}},"metadata":{}},{"name":"stdout","text":"Dataset size: 727\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b2636e1aa22493392ec2c6331abd8d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5166e0b3e724404b8e12825fb31eeee1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85024d1ebb344273a863f2c59c0e2cec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796f577567304a08ba4c69523d2619d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/581 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8faeb38c75d247378056d02188ce87ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/146 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5089aee6206f48aca2479b9b8d667be2"}},"metadata":{}},{"name":"stderr","text":"[I 2025-04-07 21:19:21,082] A new study created in memory with name: no-name-0b553d53-fcda-4d8e-8819-b7109790cc4c\n","output_type":"stream"},{"name":"stdout","text":"pos_weight: tensor([9.0056e-02, 3.0142e-02, 2.2833e-01, 2.6667e+01, 3.5391e+00, 9.3667e-01,\n        0.0000e+00, 2.8950e+02, 1.1520e+02, 9.5833e+01, 1.1946e-01, 1.2261e+00,\n        4.7462e-01, 6.3544e+00, 1.5507e-01, 2.3977e+00, 3.4542e-03, 6.0000e+00,\n        7.5441e+00, 7.1831e+00, 1.9267e+02, 7.8030e+00, 2.6541e+00, 2.3200e+00,\n        1.2379e-01, 5.3298e-01, 1.5600e+01, 1.2195e-02, 1.0857e+01, 5.0633e-02,\n        9.3750e+00, 1.6606e+01, 3.4180e-01, 5.8100e+10])\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-e5ce33bc2818>:109: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1.5e-05, 2.5e-05)\n<ipython-input-1-e5ce33bc2818>:110: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 0.001, 0.002)\n<ipython-input-1-e5ce33bc2818>:113: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  threshold = trial.suggest_uniform(\"threshold\", 0.46, 0.48)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"144c432f319741dca9526f7005807e50"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-1-e5ce33bc2818>:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MyTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='228' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [228/228 04:23, Epoch 12/12]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Micro F1</th>\n      <th>Macro F1</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.774900</td>\n      <td>0.803360</td>\n      <td>0.529515</td>\n      <td>0.397882</td>\n      <td>0.529515</td>\n      <td>0.390883</td>\n      <td>0.820526</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.820100</td>\n      <td>0.802669</td>\n      <td>0.529685</td>\n      <td>0.398018</td>\n      <td>0.529685</td>\n      <td>0.391068</td>\n      <td>0.820526</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.751100</td>\n      <td>0.806021</td>\n      <td>0.593634</td>\n      <td>0.408965</td>\n      <td>0.593634</td>\n      <td>0.473575</td>\n      <td>0.795240</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.771000</td>\n      <td>0.798266</td>\n      <td>0.580411</td>\n      <td>0.415773</td>\n      <td>0.580411</td>\n      <td>0.445387</td>\n      <td>0.832920</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.788300</td>\n      <td>0.799872</td>\n      <td>0.603980</td>\n      <td>0.429615</td>\n      <td>0.603980</td>\n      <td>0.497909</td>\n      <td>0.767476</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.755500</td>\n      <td>0.799897</td>\n      <td>0.609932</td>\n      <td>0.426891</td>\n      <td>0.609932</td>\n      <td>0.493710</td>\n      <td>0.797719</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.752500</td>\n      <td>0.814325</td>\n      <td>0.614356</td>\n      <td>0.435988</td>\n      <td>0.614356</td>\n      <td>0.509126</td>\n      <td>0.774417</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.770000</td>\n      <td>0.809771</td>\n      <td>0.645767</td>\n      <td>0.454893</td>\n      <td>0.645767</td>\n      <td>0.540622</td>\n      <td>0.801686</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.788600</td>\n      <td>0.804306</td>\n      <td>0.678496</td>\n      <td>0.462163</td>\n      <td>0.678496</td>\n      <td>0.591093</td>\n      <td>0.796232</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.755200</td>\n      <td>0.791959</td>\n      <td>0.679470</td>\n      <td>0.462040</td>\n      <td>0.679470</td>\n      <td>0.612415</td>\n      <td>0.763014</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.712700</td>\n      <td>0.793221</td>\n      <td>0.677562</td>\n      <td>0.460386</td>\n      <td>0.677562</td>\n      <td>0.610912</td>\n      <td>0.760535</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.728300</td>\n      <td>0.789178</td>\n      <td>0.695494</td>\n      <td>0.468176</td>\n      <td>0.695494</td>\n      <td>0.642827</td>\n      <td>0.757561</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-04-07 21:23:53,181] Trial 0 finished with value: 0.6954938552571689 and parameters: {'learning_rate': 1.9768352923720516e-05, 'weight_decay': 0.0013309036353077938, 'batch_size': 16, 'num_train_epochs': 12, 'threshold': 0.46269254347612143}. Best is trial 0 with value: 0.6954938552571689.\n","output_type":"stream"},{"name":"stdout","text":"Trial finished: f1: 0.6955, params: {'learning_rate': 1.9768352923720516e-05, 'weight_decay': 0.0013309036353077938, 'batch_size': 16, 'num_train_epochs': 12, 'threshold': 0.46269254347612143}\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-e5ce33bc2818>:109: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1.5e-05, 2.5e-05)\n<ipython-input-1-e5ce33bc2818>:110: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 0.001, 0.002)\n<ipython-input-1-e5ce33bc2818>:113: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  threshold = trial.suggest_uniform(\"threshold\", 0.46, 0.48)\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-1-e5ce33bc2818>:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MyTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='266' max='266' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [266/266 05:09, Epoch 14/14]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Micro F1</th>\n      <th>Macro F1</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.778900</td>\n      <td>0.792225</td>\n      <td>0.443621</td>\n      <td>0.305038</td>\n      <td>0.443621</td>\n      <td>0.344384</td>\n      <td>0.623203</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.826600</td>\n      <td>0.791836</td>\n      <td>0.443621</td>\n      <td>0.305038</td>\n      <td>0.443621</td>\n      <td>0.344384</td>\n      <td>0.623203</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.758500</td>\n      <td>0.795948</td>\n      <td>0.482553</td>\n      <td>0.368795</td>\n      <td>0.482553</td>\n      <td>0.362035</td>\n      <td>0.723352</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.780100</td>\n      <td>0.838335</td>\n      <td>0.583957</td>\n      <td>0.404741</td>\n      <td>0.583957</td>\n      <td>0.455886</td>\n      <td>0.812097</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.795800</td>\n      <td>0.838281</td>\n      <td>0.614790</td>\n      <td>0.418979</td>\n      <td>0.614790</td>\n      <td>0.488739</td>\n      <td>0.828458</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.766200</td>\n      <td>0.826467</td>\n      <td>0.630265</td>\n      <td>0.429782</td>\n      <td>0.630265</td>\n      <td>0.504927</td>\n      <td>0.838374</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.760200</td>\n      <td>0.824663</td>\n      <td>0.602195</td>\n      <td>0.422562</td>\n      <td>0.602195</td>\n      <td>0.481845</td>\n      <td>0.802677</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.756500</td>\n      <td>0.811195</td>\n      <td>0.629459</td>\n      <td>0.415227</td>\n      <td>0.629459</td>\n      <td>0.513294</td>\n      <td>0.813585</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.795100</td>\n      <td>0.822581</td>\n      <td>0.642353</td>\n      <td>0.425836</td>\n      <td>0.642353</td>\n      <td>0.541073</td>\n      <td>0.790283</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.787900</td>\n      <td>0.807964</td>\n      <td>0.676682</td>\n      <td>0.450097</td>\n      <td>0.676682</td>\n      <td>0.617683</td>\n      <td>0.748141</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.726500</td>\n      <td>0.793292</td>\n      <td>0.683247</td>\n      <td>0.458270</td>\n      <td>0.683247</td>\n      <td>0.628013</td>\n      <td>0.749132</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.743900</td>\n      <td>0.769739</td>\n      <td>0.667098</td>\n      <td>0.454141</td>\n      <td>0.667098</td>\n      <td>0.590233</td>\n      <td>0.766981</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.708600</td>\n      <td>0.764050</td>\n      <td>0.667265</td>\n      <td>0.456369</td>\n      <td>0.667265</td>\n      <td>0.609426</td>\n      <td>0.737234</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.648300</td>\n      <td>0.741273</td>\n      <td>0.680952</td>\n      <td>0.488926</td>\n      <td>0.680952</td>\n      <td>0.604303</td>\n      <td>0.779871</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-04-07 21:29:06,655] Trial 1 finished with value: 0.680952380952381 and parameters: {'learning_rate': 1.935522882724049e-05, 'weight_decay': 0.0012100309698722834, 'batch_size': 16, 'num_train_epochs': 14, 'threshold': 0.47022289494356106}. Best is trial 0 with value: 0.6954938552571689.\n","output_type":"stream"},{"name":"stdout","text":"Trial finished: f1: 0.6810, params: {'learning_rate': 1.935522882724049e-05, 'weight_decay': 0.0012100309698722834, 'batch_size': 16, 'num_train_epochs': 14, 'threshold': 0.47022289494356106}\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-e5ce33bc2818>:109: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1.5e-05, 2.5e-05)\n<ipython-input-1-e5ce33bc2818>:110: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 0.001, 0.002)\n<ipython-input-1-e5ce33bc2818>:113: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  threshold = trial.suggest_uniform(\"threshold\", 0.46, 0.48)\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-1-e5ce33bc2818>:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MyTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [190/190 03:40, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Micro F1</th>\n      <th>Macro F1</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.768700</td>\n      <td>0.811689</td>\n      <td>0.536068</td>\n      <td>0.372062</td>\n      <td>0.536068</td>\n      <td>0.412741</td>\n      <td>0.764502</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.824300</td>\n      <td>0.811129</td>\n      <td>0.536602</td>\n      <td>0.372183</td>\n      <td>0.536602</td>\n      <td>0.413230</td>\n      <td>0.764998</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.753600</td>\n      <td>0.810476</td>\n      <td>0.536416</td>\n      <td>0.372062</td>\n      <td>0.536416</td>\n      <td>0.413009</td>\n      <td>0.764998</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.782600</td>\n      <td>0.805453</td>\n      <td>0.552078</td>\n      <td>0.381051</td>\n      <td>0.552078</td>\n      <td>0.437609</td>\n      <td>0.747645</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.800100</td>\n      <td>0.797942</td>\n      <td>0.623083</td>\n      <td>0.412506</td>\n      <td>0.623083</td>\n      <td>0.525349</td>\n      <td>0.765493</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.765500</td>\n      <td>0.802421</td>\n      <td>0.646513</td>\n      <td>0.450257</td>\n      <td>0.646513</td>\n      <td>0.516444</td>\n      <td>0.864155</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.752900</td>\n      <td>0.804853</td>\n      <td>0.649579</td>\n      <td>0.450671</td>\n      <td>0.649579</td>\n      <td>0.514949</td>\n      <td>0.879524</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.763600</td>\n      <td>0.794651</td>\n      <td>0.715002</td>\n      <td>0.465661</td>\n      <td>0.715002</td>\n      <td>0.617913</td>\n      <td>0.848290</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.788700</td>\n      <td>0.792752</td>\n      <td>0.709064</td>\n      <td>0.470259</td>\n      <td>0.709064</td>\n      <td>0.617820</td>\n      <td>0.831929</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.758100</td>\n      <td>0.781533</td>\n      <td>0.695452</td>\n      <td>0.456620</td>\n      <td>0.695452</td>\n      <td>0.647839</td>\n      <td>0.750620</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-04-07 21:32:51,181] Trial 2 finished with value: 0.6954524575103354 and parameters: {'learning_rate': 1.984973147517844e-05, 'weight_decay': 0.0011163000423725223, 'batch_size': 16, 'num_train_epochs': 10, 'threshold': 0.4795055992267837}. Best is trial 0 with value: 0.6954938552571689.\n","output_type":"stream"},{"name":"stdout","text":"Trial finished: f1: 0.6955, params: {'learning_rate': 1.984973147517844e-05, 'weight_decay': 0.0011163000423725223, 'batch_size': 16, 'num_train_epochs': 10, 'threshold': 0.4795055992267837}\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-e5ce33bc2818>:109: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1.5e-05, 2.5e-05)\n<ipython-input-1-e5ce33bc2818>:110: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 0.001, 0.002)\n<ipython-input-1-e5ce33bc2818>:113: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  threshold = trial.suggest_uniform(\"threshold\", 0.46, 0.48)\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-1-e5ce33bc2818>:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MyTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='266' max='266' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [266/266 05:09, Epoch 14/14]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Micro F1</th>\n      <th>Macro F1</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.778600</td>\n      <td>0.795593</td>\n      <td>0.512531</td>\n      <td>0.378553</td>\n      <td>0.512531</td>\n      <td>0.385230</td>\n      <td>0.765493</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.824800</td>\n      <td>0.795222</td>\n      <td>0.510785</td>\n      <td>0.379842</td>\n      <td>0.510785</td>\n      <td>0.382396</td>\n      <td>0.768964</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.760300</td>\n      <td>0.794560</td>\n      <td>0.502835</td>\n      <td>0.379025</td>\n      <td>0.502835</td>\n      <td>0.373436</td>\n      <td>0.769460</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.781600</td>\n      <td>0.795420</td>\n      <td>0.570731</td>\n      <td>0.389894</td>\n      <td>0.570731</td>\n      <td>0.459468</td>\n      <td>0.753099</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.790600</td>\n      <td>0.794237</td>\n      <td>0.605126</td>\n      <td>0.399301</td>\n      <td>0.605126</td>\n      <td>0.507558</td>\n      <td>0.749132</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.753400</td>\n      <td>0.790742</td>\n      <td>0.579825</td>\n      <td>0.392888</td>\n      <td>0.579825</td>\n      <td>0.483604</td>\n      <td>0.723847</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.763800</td>\n      <td>0.795398</td>\n      <td>0.618434</td>\n      <td>0.421323</td>\n      <td>0.618434</td>\n      <td>0.515192</td>\n      <td>0.773426</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.755800</td>\n      <td>0.796930</td>\n      <td>0.626327</td>\n      <td>0.421142</td>\n      <td>0.626327</td>\n      <td>0.547716</td>\n      <td>0.731284</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.790000</td>\n      <td>0.793508</td>\n      <td>0.629007</td>\n      <td>0.411611</td>\n      <td>0.629007</td>\n      <td>0.574059</td>\n      <td>0.695588</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.753400</td>\n      <td>0.779116</td>\n      <td>0.677583</td>\n      <td>0.443494</td>\n      <td>0.677583</td>\n      <td>0.633463</td>\n      <td>0.728309</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.719000</td>\n      <td>0.785821</td>\n      <td>0.687426</td>\n      <td>0.445022</td>\n      <td>0.687426</td>\n      <td>0.655716</td>\n      <td>0.722360</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.738800</td>\n      <td>0.769771</td>\n      <td>0.660225</td>\n      <td>0.425419</td>\n      <td>0.660225</td>\n      <td>0.615880</td>\n      <td>0.711453</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.719000</td>\n      <td>0.772709</td>\n      <td>0.697484</td>\n      <td>0.488726</td>\n      <td>0.697484</td>\n      <td>0.615793</td>\n      <td>0.804165</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.674400</td>\n      <td>0.739237</td>\n      <td>0.679245</td>\n      <td>0.474097</td>\n      <td>0.679245</td>\n      <td>0.598413</td>\n      <td>0.785325</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-04-07 21:38:04,583] Trial 3 finished with value: 0.6792452830188679 and parameters: {'learning_rate': 1.775451565900352e-05, 'weight_decay': 0.0010565936969682268, 'batch_size': 16, 'num_train_epochs': 14, 'threshold': 0.4722450053273777}. Best is trial 0 with value: 0.6954938552571689.\n","output_type":"stream"},{"name":"stdout","text":"Trial finished: f1: 0.6792, params: {'learning_rate': 1.775451565900352e-05, 'weight_decay': 0.0010565936969682268, 'batch_size': 16, 'num_train_epochs': 14, 'threshold': 0.4722450053273777}\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-e5ce33bc2818>:109: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1.5e-05, 2.5e-05)\n<ipython-input-1-e5ce33bc2818>:110: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  weight_decay = trial.suggest_loguniform(\"weight_decay\", 0.001, 0.002)\n<ipython-input-1-e5ce33bc2818>:113: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  threshold = trial.suggest_uniform(\"threshold\", 0.46, 0.48)\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-1-e5ce33bc2818>:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MyTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='266' max='266' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [266/266 05:09, Epoch 14/14]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Micro F1</th>\n      <th>Macro F1</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.768700</td>\n      <td>0.811706</td>\n      <td>0.533379</td>\n      <td>0.371515</td>\n      <td>0.533379</td>\n      <td>0.409562</td>\n      <td>0.764502</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.824300</td>\n      <td>0.811241</td>\n      <td>0.534372</td>\n      <td>0.371669</td>\n      <td>0.534372</td>\n      <td>0.410591</td>\n      <td>0.764998</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.753900</td>\n      <td>0.810753</td>\n      <td>0.535671</td>\n      <td>0.371898</td>\n      <td>0.535671</td>\n      <td>0.412126</td>\n      <td>0.764998</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.782400</td>\n      <td>0.808455</td>\n      <td>0.532653</td>\n      <td>0.370293</td>\n      <td>0.532653</td>\n      <td>0.412937</td>\n      <td>0.750124</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.799200</td>\n      <td>0.798293</td>\n      <td>0.609328</td>\n      <td>0.403607</td>\n      <td>0.609328</td>\n      <td>0.513965</td>\n      <td>0.748141</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.769800</td>\n      <td>0.803784</td>\n      <td>0.638646</td>\n      <td>0.438038</td>\n      <td>0.638646</td>\n      <td>0.511012</td>\n      <td>0.851264</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.754700</td>\n      <td>0.807223</td>\n      <td>0.645405</td>\n      <td>0.449447</td>\n      <td>0.645405</td>\n      <td>0.506935</td>\n      <td>0.887952</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.765900</td>\n      <td>0.805119</td>\n      <td>0.705287</td>\n      <td>0.461114</td>\n      <td>0.705287</td>\n      <td>0.596233</td>\n      <td>0.863163</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.790000</td>\n      <td>0.799479</td>\n      <td>0.700141</td>\n      <td>0.464979</td>\n      <td>0.700141</td>\n      <td>0.588454</td>\n      <td>0.864155</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.770500</td>\n      <td>0.797151</td>\n      <td>0.707658</td>\n      <td>0.448714</td>\n      <td>0.707658</td>\n      <td>0.648370</td>\n      <td>0.778880</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.725300</td>\n      <td>0.786062</td>\n      <td>0.684704</td>\n      <td>0.470263</td>\n      <td>0.684704</td>\n      <td>0.611458</td>\n      <td>0.777888</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.743800</td>\n      <td>0.780523</td>\n      <td>0.687611</td>\n      <td>0.464338</td>\n      <td>0.687611</td>\n      <td>0.624444</td>\n      <td>0.764998</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.710700</td>\n      <td>0.773906</td>\n      <td>0.683415</td>\n      <td>0.476399</td>\n      <td>0.683415</td>\n      <td>0.619508</td>\n      <td>0.762023</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.664000</td>\n      <td>0.762127</td>\n      <td>0.680352</td>\n      <td>0.481360</td>\n      <td>0.680352</td>\n      <td>0.624172</td>\n      <td>0.747645</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-04-07 21:43:17,845] Trial 4 finished with value: 0.6803519061583578 and parameters: {'learning_rate': 1.6466692666350026e-05, 'weight_decay': 0.0012022180401906955, 'batch_size': 16, 'num_train_epochs': 14, 'threshold': 0.4791430511285427}. Best is trial 0 with value: 0.6954938552571689.\n","output_type":"stream"},{"name":"stdout","text":"Trial finished: f1: 0.6804, params: {'learning_rate': 1.6466692666350026e-05, 'weight_decay': 0.0012022180401906955, 'batch_size': 16, 'num_train_epochs': 14, 'threshold': 0.4791430511285427}\nBest trial:\n  Value:  0.6954938552571689\n  Params: \n    learning_rate: 1.9768352923720516e-05\n    weight_decay: 0.0013309036353077938\n    batch_size: 16\n    num_train_epochs: 12\n    threshold: 0.46269254347612143\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-e5ce33bc2818>:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MyTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='228' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [228/228 06:12, Epoch 12/12]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Micro F1</th>\n      <th>Macro F1</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.768900</td>\n      <td>0.813504</td>\n      <td>0.494814</td>\n      <td>0.363229</td>\n      <td>0.494814</td>\n      <td>0.373391</td>\n      <td>0.733267</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.822400</td>\n      <td>0.813217</td>\n      <td>0.496225</td>\n      <td>0.363229</td>\n      <td>0.496225</td>\n      <td>0.375000</td>\n      <td>0.733267</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.749800</td>\n      <td>0.815925</td>\n      <td>0.516815</td>\n      <td>0.380462</td>\n      <td>0.516815</td>\n      <td>0.395172</td>\n      <td>0.746653</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.773500</td>\n      <td>0.819981</td>\n      <td>0.548018</td>\n      <td>0.397034</td>\n      <td>0.548018</td>\n      <td>0.425096</td>\n      <td>0.770947</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.798300</td>\n      <td>0.816580</td>\n      <td>0.574476</td>\n      <td>0.410430</td>\n      <td>0.574476</td>\n      <td>0.452063</td>\n      <td>0.787804</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.754700</td>\n      <td>0.815677</td>\n      <td>0.566860</td>\n      <td>0.417941</td>\n      <td>0.566860</td>\n      <td>0.447376</td>\n      <td>0.773426</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.748700</td>\n      <td>0.819297</td>\n      <td>0.590204</td>\n      <td>0.441485</td>\n      <td>0.590204</td>\n      <td>0.457775</td>\n      <td>0.830441</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.759300</td>\n      <td>0.815938</td>\n      <td>0.666923</td>\n      <td>0.462913</td>\n      <td>0.666923</td>\n      <td>0.544371</td>\n      <td>0.860684</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.792800</td>\n      <td>0.809999</td>\n      <td>0.694073</td>\n      <td>0.489557</td>\n      <td>0.694073</td>\n      <td>0.580828</td>\n      <td>0.862172</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.754800</td>\n      <td>0.791652</td>\n      <td>0.691312</td>\n      <td>0.487775</td>\n      <td>0.691312</td>\n      <td>0.590112</td>\n      <td>0.834408</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.717400</td>\n      <td>0.775410</td>\n      <td>0.673949</td>\n      <td>0.482349</td>\n      <td>0.673949</td>\n      <td>0.576720</td>\n      <td>0.810610</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.716300</td>\n      <td>0.774263</td>\n      <td>0.693663</td>\n      <td>0.479093</td>\n      <td>0.693663</td>\n      <td>0.601310</td>\n      <td>0.819534</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final evaluation results: {'eval_micro_f1': 0.6940730393135103, 'eval_macro_f1': 0.4895570593968406, 'eval_f1': 0.6940730393135103, 'eval_precision': 0.5808283233132933, 'eval_recall': 0.8621715418939019, 'eval_loss': 0.8099992275238037, 'eval_runtime': 1.6517, 'eval_samples_per_second': 88.393, 'eval_steps_per_second': 3.027, 'epoch': 12.0}\nBest model saved to ./Rock\n  adding: Rock/ (stored 0%)\n  adding: Rock/config.json (deflated 67%)\n  adding: Rock/sentencepiece.bpe.model (deflated 49%)\n  adding: Rock/training_args.bin (deflated 51%)\n  adding: Rock/special_tokens_map.json (deflated 52%)\n  adding: Rock/model.safetensors (deflated 30%)\n  adding: Rock/tokenizer.json (deflated 76%)\n  adding: Rock/tokenizer_config.json (deflated 76%)\nRock.zip created. Download it from the Output tab in Kaggle.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(\"Current THRESHOLD:\", THRESHOLD)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T22:19:48.078992Z","iopub.execute_input":"2025-04-07T22:19:48.079367Z","iopub.status.idle":"2025-04-07T22:19:48.084324Z","shell.execute_reply.started":"2025-04-07T22:19:48.079321Z","shell.execute_reply":"2025-04-07T22:19:48.083314Z"}},"outputs":[{"name":"stdout","text":"Current THRESHOLD: 0.46269254347612143\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}